{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModularMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(ModularMLP, self).__init__()\n",
    "        self.input_size = config['input_size']\n",
    "        self.layer_size = config['layer_size']\n",
    "        self.num_hidden_layers = config['num_hidden_size']\n",
    "        self.output_size = config['output_size']\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(self.input_size, self.layer_size))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        for _ in range(self.num_hidden_layers - 1):  # -1 because we want to add the final layer separately\n",
    "            layers.append(nn.Linear(self.layer_size, self.layer_size))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        layers.append(nn.Linear(self.layer_size, self.output_size))\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def initialize_random_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight)\n",
    "                nn.init.normal_(m.bias)\n",
    "\n",
    "# Export the model to ONNX\n",
    "def export_to_onnx(model, dummy_input, filename):\n",
    "    torch.onnx.export(model, dummy_input, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'input_size': 200,\n",
    "    'layer_size': 100,\n",
    "    'num_hidden_size': 10,\n",
    "    'output_size': 10\n",
    "}\n",
    "\n",
    "model = ModularMLP(config)\n",
    "model.initialize_random_weights()  # Initialize with random weights\n",
    "\n",
    "# Export the model\n",
    "dummy_input = torch.randn(1, config['input_size'])  # Single input tensor of shape [1, 200]\n",
    "export_to_onnx(model, dummy_input, f\"mlp_{config['num_hidden_size']}.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Scarb project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created `your_project_name` package.\n",
      "Created `model` package.\n",
      "Created `inputs` package.\n",
      "Created `inference` package.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "\n",
    "def add_dependencies(path, root=True):\n",
    "    filepath = f\"{path}/Scarb.toml\"\n",
    "    with open(filepath, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Search for the [dependencies] section\n",
    "    index = -1\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip() == \"[dependencies]\":\n",
    "            index = i\n",
    "            break\n",
    "\n",
    "    # Add the dependencies after the [dependencies] section\n",
    "    if index != -1:\n",
    "        dependencies = [\n",
    "            'orion = { git = \"https://github.com/gizatechxyz/orion.git\", rev = \"v0.1.1\" }\\n',]\n",
    "\n",
    "        if root:\n",
    "            dependencies.append(\n",
    "                'model = { path = \"model\" }\\n inputs = { path = \"inputs\" }\\n inference = { path = \"inference\" }\\n')\n",
    "        for dep in dependencies:\n",
    "            index += 1\n",
    "            lines.insert(index, dep)\n",
    "\n",
    "        with open(filepath, 'w') as file:\n",
    "            file.writelines(lines)\n",
    "    else:\n",
    "        print(f\"Could not find [dependencies] section in {filepath}\")\n",
    "\n",
    "\n",
    "def create_scarb_project(name):\n",
    "    root = f\"scarb new {name}\"\n",
    "    subprocess.run(root, shell=True, check=True)\n",
    "    commands = f\"cd {name} && scarb new model && scarb new inputs && scarb new inference\"\n",
    "    subprocess.run(commands, shell=True, check=True)\n",
    "    add_dependencies(name)\n",
    "    add_dependencies(f\"{name}/model\", False)\n",
    "    add_dependencies(f\"{name}/inputs\", False)\n",
    "    add_dependencies(f\"{name}/inference\", False)\n",
    "    os.makedirs(f\"{name}/model/src/weights\", exist_ok=True)\n",
    "    os.makedirs(f\"{name}/model/src/biases\", exist_ok=True)\n",
    "\n",
    "\n",
    "def write_tensor_to_cairo(tensor, tensor_name, directory):\n",
    "    with open(os.path.join(directory, f\"{tensor_name}.cairo\"), \"w\") as f:\n",
    "        f.write(\n",
    "            \"use array::ArrayTrait;\\n\" +\n",
    "            \"use orion::operators::tensor::{TensorTrait, Tensor, FP16x16Tensor};\\n\" +\n",
    "            \"use orion::numbers::FP16x16;\\n\\n\" +\n",
    "            \"\\nfn {0}() -> Tensor<FP16x16> \".format(tensor_name) + \"{\\n\" +\n",
    "            \"    let mut shape = ArrayTrait::<usize>::new();\\n\"\n",
    "        )\n",
    "        for dim in tensor.shape:\n",
    "            f.write(\"    shape.append({0});\\n\".format(dim))\n",
    "        f.write(\n",
    "            \"    let mut data = ArrayTrait::<FP16x16>::new();\\n\"\n",
    "        )\n",
    "        for val in tensor.flatten():\n",
    "            val_fp = int(val.item() * 2 ** 16)\n",
    "            f.write(\"    data.append(FP16x16 {{ mag: {0}, sign: {1} }});\\n\".format(\n",
    "                abs(val_fp), str(val_fp < 0).lower()))\n",
    "        f.write(\n",
    "            \"    TensorTrait::new(shape.span(), data.span())\\n\" +\n",
    "            \"}\\n\"\n",
    "        )\n",
    "\n",
    "def generate_mod_file(directory):\n",
    "    with open(f\"{directory}.cairo\", \"w\") as f:\n",
    "        for item_name in os.listdir(directory):\n",
    "            # Exclude if it's a directory\n",
    "            if not os.path.isdir(f\"{directory}/{item_name}\"):\n",
    "                module_name = item_name.split('.')[0]  # removing file extension\n",
    "                f.write(f\"mod {module_name};\\n\")\n",
    "\n",
    "\n",
    "def write_tensors(model, dummy_input, project_name):\n",
    "    input_tensor = dummy_input[0].flatten()\n",
    "    write_tensor_to_cairo(input_tensor, \"input\", f\"{project_name}/inputs/src\")\n",
    "\n",
    "    for idx, m in enumerate(model.modules()):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            layer_name = f\"layer_{idx}\"\n",
    "            weight_name = f\"{layer_name}_weights\"\n",
    "            bias_name = f\"{layer_name}_bias\"\n",
    "\n",
    "            weight_dir = f\"{project_name}/model/src/weights\"\n",
    "            bias_dir = f\"{project_name}/model/src/biases\"\n",
    "            \n",
    "            # Writing weights and biases\n",
    "            write_tensor_to_cairo(m.weight, weight_name, weight_dir)\n",
    "            write_tensor_to_cairo(m.bias, bias_name, bias_dir)\n",
    "\n",
    "            # Generating mod file\n",
    "            generate_mod_file(weight_dir)\n",
    "            generate_mod_file(bias_dir)\n",
    "\n",
    "\n",
    "def write_in_lib(path, content):\n",
    "     with open(f\"{path}.cairo\", \"w\") as f:\n",
    "         f.write(content)\n",
    "         \n",
    "def write_inference(project_name):\n",
    "    with open(f\"{project_name}/inference/src/lib.cairo\", \"w\") as f:\n",
    "            f.write()\n",
    "        for _ in range(config['num_hidden_size'] - 1):\n",
    "            f.write()\n",
    "\n",
    "# Usage:\n",
    "project_name = \"your_project_name\"\n",
    "create_scarb_project(project_name)\n",
    "write_tensors(model, dummy_input, project_name)\n",
    "write_in_lib(f\"{project_name}/model/src/lib\" ,\"mod weights;\\nmod biases;\") #model\n",
    "write_in_lib(f\"{project_name}/inputs/src/lib\", \"mod input;\") #input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
